---
title: 机器学习-朴素贝叶斯
date: 2017-06-06 19:23:52
tags: scikit-learn
categories: 机器学习
---

## 贝叶斯
### 贝叶斯推断：先估计一个值，根据实际结果不断修正。
### 贝叶斯定理：P(A|B)，条件概率，事件B发生的情况下，事件A发生的概率。

![文氏图](机器学习-朴素贝叶斯/venturi-diagram.jpg)
由文氏图得
{% katex [displayMode] %}

P \left ( A|B \right ) = \frac {P \left ( A \bigcap B \right )} {P \left ( B \right )}

{% endkatex %}
又
{% katex [displayMode] %}

P \left ( B|A \right ) = \frac {P \left ( A \bigcap B \right )} {P \left ( B \right )}

{% endkatex %}
故
{% katex [displayMode] %}

P \left ( A \bigcap B \right ) = P \left ( A|B \right ) \ast P \left ( B \right ) = P \left ( B|A \right ) \ast P \left ( A \right )

{% endkatex %}
最终
{% katex [displayMode] %}

P \left ( A|B \right ) = \frac{P \left ( B|A \right ) \ast P \left ( A \right )}{P \left ( B \right )}

{% endkatex %}
### 全概率公式
由文氏图得
{% katex [displayMode] %}

P \left ( B \right ) = P \left ( B \bigcap A \right ) + P \left (  B \bigcap{A}' \right )

{% endkatex %}
由条件概率公式得
{% katex [displayMode] %}

P \left ( B \bigcap A \right ) = P \left ( B|A \right ) \ast P \left ( A \right )

{% endkatex %}
代入得全概率公式
{% katex [displayMode] %}

P \left ( B \right ) = P \left ( B|A \right ) \ast P \left ( A \right ) + P \left ( B|{A}' \right ) \ast P \left ( {A}' \right )

{% endkatex %}
代入条件概率公式得到条件概率另一种写法

{% katex [displayMode] %}

P \left ( A|B \right ) = \frac{P \left ( B|A \right ) \ast P \left ( A \right )}{P \left ( B \right )} = \frac{P\left ( B|A \right )\ast P\left ( A \right )}{P \left ( B|A \right ) \ast P \left ( A \right ) + P \left ( B|{A}' \right ) \ast P \left ( {A}' \right )}

{% endkatex %}
### 贝叶斯推断
条件概率
{% katex [displayMode] %}

P \left ( A|B \right ) = P \left ( A \right ) \ast \frac{P \left ( B|A \right )}{P \left ( B \right )}

{% endkatex %}
P(A)：先验概率，在事件B发生之前，事件A发生的概率，与事件B无关。
P(A|B)：后验概率，在事件B发生之后，事件A发生的概率。
P(B|A)/P(B)：可能性函数，用于调整先验概率，使之接近后验概率。若大于1，先验概率被增强，后验概率变大，反之，先验概率被削弱，后验概率变小。

## 使用贝叶斯过滤垃圾邮件
假定先验概率，垃圾邮件的概率P(S)=50%，正常邮件的概率P(H)=50%
根据条件概率公式，某个词语存在的条件下，垃圾邮件的概率：
{% katex [displayMode] %}

P \left ( S|W \right ) = \frac{P\left ( W|S \right ) \ast P\left ( S \right )}{P\left ( W|S \right )\ast P\left ( S \right )+P\left ( W|H \right )\ast P\left ( H \right )}

{% endkatex %}
需要求出P(W|S)垃圾邮件中W单词的概率和P(W|H)正常邮件中W单词的概率
一般情况下我们需要根据多个单词的出现判断垃圾邮件
P1 = P(S|W1)
P2 = P(S|W2)

| 事件 | 单词W1 | 单词W2 | 是否是垃圾邮件 |
|:--------|:---------|:-------|:-------|
| 垃圾邮件E1 | P1 | P2 | P(S) |
| 正常邮件E2 | 1-P1 | 1-P2 | 1-P(S) |

假设事件独立
P(E1) = P(S|W1)P(S|W2)P(S)
P(E2) = (1-P(S|W1))(1-P(S|W2))(1-P(S))
{% katex [displayMode] %}
P=\frac{P\left ( E1 \right )}{P\left ( E1 \right ) + P\left ( E2 \right )}=\frac{P(S|W1)P(S|W2)P(S)}{P(S|W1)P(S|W2)P(S)+(1-P(S|W1)(1-P(S|W2)(1-P(S))))}
{% endkatex %}
代入P(S)=0.5
{% katex [displayMode] %}
P=\frac{P(S|W1)P(S|W2)}{P(S|W1)P(S|W2)+(1-P(S|W1)(1-P(S|W2)))}
{% endkatex %}
{% katex [displayMode] %}
P=\frac{P1P2}{P1P2 + (1-P1)(1-P2)}
{% endkatex %}
扩展到所有单词
{% katex [displayMode] %}
P=\frac{P1P2...Pn}{P1P2...Pn + (1-P1)(1-P2)...(1-Pn)}
{% endkatex %}

## 朴素贝叶斯三个模型（scikit-learn）
高斯Gaussian:特征值分布符合高斯分布
多项式Multinomial:适合文本分类
伯努利Bernoulli:特征取值是布尔型的
<!--{% katex [displayMode] %}
P(x_{i}|y)= P(i|y)x_{i} + (1-P(i|y))(1-x_{i})
{% endkatex %}-->

